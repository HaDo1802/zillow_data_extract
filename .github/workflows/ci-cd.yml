name: Real Estate ETL Pipeline CI/CD

on:
  #schedule:
    #- cron: "0 6 * * *" # Daily at 6 AM UTC
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      run_full_pipeline:
        description: "Run full ETL pipeline (not just tests)"
        required: false
        default: "false"
        type: choice
        options:
          - "true"
          - "false"

env:
  PYTHON_VERSION: "3.9"

jobs:
  format-and-lint:
    name: Code Format & Lint
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install formatting tools
        run: |
          python -m pip install --upgrade pip
          pip install black isort

      - name: Format code with Black
        run: |
          echo "üé® Formatting code with Black..."
          black etl/ tests/ --line-length 127

      - name: Sort imports with isort
        run: |
          echo "üì¶ Sorting imports..."
          isort etl/ tests/ --profile black

      # No flake8, no linting failures, no noise
      # No auto-commit (pre-commit handles formatting locally)

  unit-test:
    name: Run Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install pandas pytest

      - name: Run unit tests
        run: |
          pytest tests/test_*.py -v

      - name: Tests passed!
        run: echo "‚úÖ All tests passed!"

  docker-build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [format-and-lint, unit-test]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' && secrets.DOCKER_USERNAME != '' }}
          tags: ${{ secrets.DOCKER_USERNAME }}/real-estate-etl:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

  run-etl-pipeline:
    runs-on: ubuntu-latest
    name: Execute Real Estate ETL Pipeline
    needs: [format-and-lint, unit-test, docker-build]
    if: github.ref == 'refs/heads/main' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.run_full_pipeline == 'true'))

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests python-dotenv boto3

      - name: Create data directories
        run: |
          mkdir -p data/raw data/transformed etl_log
          chmod 777 data/raw data/transformed etl_log

      - name: Configure environment
        run: |
          # API Key (required for extract.py)
          echo "RAPID_API_KEY=${{ secrets.RAPID_API_KEY }}" > .env
          echo "‚úÖ Environment configured for extraction + transform"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Step 1 - Extract Real Estate Data
        run: |
          echo "üï∑Ô∏è Starting Zillow data extraction..."
          cd etl
          python extract.py
          echo "‚úÖ Data extraction completed"

          if [ -f "../data/raw/raw_latest.csv" ]; then
            echo "üìÑ Raw data file created successfully"
            echo "Raw data rows: $(wc -l < ../data/raw/raw_latest.csv)"
          else
            echo "‚ùå Raw data file not found"
            exit 1
          fi

      - name: Step 2 - Transform and Clean Data
        run: |
          echo "üßπ Starting data transformation..."
          cd etl
          python transform.py
          echo "‚úÖ Data transformation completed"

          if [ -f "../data/transformed/transformed_latest.csv" ]; then
            echo "üìÑ Transformed data file created successfully"
            echo "Transformed data rows: $(wc -l < ../data/transformed/transformed_latest.csv)"
          else
            echo "‚ùå Transformed data file not found"
            exit 1
          fi

      - name: Step 3 - Upload to S3
        env:
          S3_BUCKET: ${{ secrets.S3_BUCKET_NAME }}
        run: |
          if [ -z "$S3_BUCKET" ]; then
            echo "‚ùå Missing required secret: S3_BUCKET_NAME"
            exit 1
          fi

          ETL_RUN_ID=$(date +'%Y%m%d_%H%M')
          SNAPSHOT_DATE=$(date +'%Y%m%d')
          RAW_S3_KEY="raw/raw_${SNAPSHOT_DATE}_${ETL_RUN_ID}.csv"
          TRANSFORMED_S3_KEY="transformed/transformed_${SNAPSHOT_DATE}_${ETL_RUN_ID}.csv"

          echo "RAW_S3_KEY=${RAW_S3_KEY}" >> "$GITHUB_ENV"
          echo "TRANSFORMED_S3_KEY=${TRANSFORMED_S3_KEY}" >> "$GITHUB_ENV"

          echo "‚òÅÔ∏è Uploading files to s3://${S3_BUCKET}"
          python - <<'PY'
          import os
          from etl.load_to_s3 import load_to_s3

          bucket = os.environ["S3_BUCKET"]
          raw_key = os.environ["RAW_S3_KEY"]
          transformed_key = os.environ["TRANSFORMED_S3_KEY"]

          load_to_s3("data/raw/raw_latest.csv", bucket, raw_key)
          load_to_s3("data/transformed/transformed_latest.csv", bucket, transformed_key)
          PY
          echo "‚úÖ S3 upload completed"

      - name: Verify S3 Upload
        env:
          S3_BUCKET: ${{ secrets.S3_BUCKET_NAME }}
        run: |
          python - <<'PY'
          import os
          import boto3

          s3 = boto3.client("s3")
          bucket = os.environ["S3_BUCKET"]
          raw_key = os.environ["RAW_S3_KEY"]
          transformed_key = os.environ["TRANSFORMED_S3_KEY"]

          s3.head_object(Bucket=bucket, Key=raw_key)
          s3.head_object(Bucket=bucket, Key=transformed_key)

          print(f"‚úÖ Verified: s3://{bucket}/{raw_key}")
          print(f"‚úÖ Verified: s3://{bucket}/{transformed_key}")
          PY

      - name: Pipeline Summary
        env:
          S3_BUCKET: ${{ secrets.S3_BUCKET_NAME }}
        run: |
          echo "üéâ Real Estate ETL Pipeline Completed Successfully!"
          echo "üìÖ Executed on: $(date)"
          echo "üìä Pipeline Summary:"
          echo "  ‚úÖ Data Extraction (Zillow API)"
          echo "  ‚úÖ Data Transformation"
          echo "  ‚úÖ S3 Upload"

          echo ""
          echo "üì¶ Local Data Files:"
          ls -lh data/raw/raw_latest.csv 2>/dev/null || echo "  Raw data: Not found"
          ls -lh data/transformed/transformed_latest.csv 2>/dev/null || echo "  Transformed data: Not found"
          echo ""
          echo "‚òÅÔ∏è S3 Objects:"
          echo "  s3://${S3_BUCKET}/${RAW_S3_KEY}"
          echo "  s3://${S3_BUCKET}/${TRANSFORMED_S3_KEY}"

      - name: Upload Data Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: real-estate-data-${{ github.run_number }}
          path: |
            data/raw/*.csv
            data/transformed/*.csv
            etl_log/*.txt
          retention-days: 30
